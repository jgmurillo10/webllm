# WebLLM Demos - Next.js AI Applications

A collection of AI-powered applications running entirely in your browser using [WebLLM](https://github.com/mlc-ai/web-llm), built with Next.js 14, TypeScript, and Tailwind CSS.

## ğŸš€ Live Demos

- **Translation App** (`/translate`) - âœ… **FULLY WORKING**
- **Image Describer** (`/describe`) - âš ï¸ UI Complete, Model Loading Issue

## âœ… Working Features

### Translation App (`/translate`)

A fully functional real-time translation application powered by Llama 3.2 3B.

**Features:**

- ğŸŒ 12 languages supported (English, Spanish, French, German, Italian, Portuguese, Chinese, Japanese, Korean, Russian, Arabic, Hindi)
- ğŸ”„ Language swap button to quickly switch source and target languages
- âš¡ Real-time streaming translations
- ğŸ¨ Beautiful Google Translate-inspired UI
- ğŸ”’ 100% private - runs locally in browser
- ğŸ’¾ Model cached after first download (~2GB)

**Model:** Llama-3.2-3B-Instruct-q4f32_1-MLC

### Image Describer (`/describe`)

Beautiful UI for image analysis with upload functionality.

**Features:**

- ğŸ“¸ Drag & drop image upload
- ğŸ¨ Modern purple/pink gradient design
- ğŸ” Automatic vision model detection
- âš™ï¸ Customizable description prompts

**Known Issue:** Vision models (Phi-3.5-vision-instruct) crash with "exit(1)" error due to high memory requirements (~2-3GB). This is a WebGPU/browser memory limitation.

## ğŸ› ï¸ Tech Stack

- **Framework:** Next.js 14.2.15 (App Router)
- **Language:** TypeScript 5.6.3
- **Styling:** Tailwind CSS 3.x
- **AI Engine:** @mlc-ai/web-llm 0.2.79
- **Package Manager:** pnpm
- **Node Version:** 22.x

## ğŸ“¦ Getting Started

### Prerequisites

- Node.js 22.x (managed via nvm)
- pnpm

### Installation

```bash
# Use Node 22
nvm use 22

# Install dependencies
pnpm install

# Run development server
pnpm dev
```

The app will be available at `http://localhost:3000` (or 3001/3002 if ports are in use).

### Available Scripts

```bash
pnpm dev     # Start development server
pnpm build   # Build for production
pnpm start   # Start production server
pnpm lint    # Run ESLint
```

## ğŸ¯ How It Works

### WebLLM Integration

Both apps use WebLLM to run AI models entirely in the browser:

1. **First Load:** Downloads and caches the model (~2GB)
2. **Subsequent Loads:** Loads from browser cache instantly
3. **Inference:** Runs on your GPU via WebGPU
4. **Privacy:** All processing happens locally, no data sent to servers

### Browser Requirements

- âœ… Chrome 113+ (WebGPU enabled by default)
- âœ… Edge 113+ (WebGPU enabled by default)
- âŒ Safari (WebGPU support limited)
- âŒ Firefox (WebGPU behind flag)

## ğŸ“ Project Structure

```
webllm/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ describe/
â”‚   â”‚   â””â”€â”€ page.tsx          # Image describer (vision model)
â”‚   â”œâ”€â”€ translate/
â”‚   â”‚   â””â”€â”€ page.tsx          # Translation app
â”‚   â”œâ”€â”€ globals.css           # Global styles + Tailwind
â”‚   â”œâ”€â”€ layout.tsx            # Root layout
â”‚   â””â”€â”€ page.tsx              # Home page with navigation
â”œâ”€â”€ tailwind.config.js        # Tailwind configuration
â”œâ”€â”€ tsconfig.json             # TypeScript configuration
â”œâ”€â”€ next.config.js            # Next.js configuration
â””â”€â”€ package.json              # Dependencies
```

## ğŸ› Known Issues

### Vision Models Not Loading

**Issue:** Phi-3.5-vision models crash with "Program terminated with exit(1)"

**Cause:**

- Vision models are very large (~2-3GB)
- WebGPU has memory limitations in browsers
- Browser may run out of GPU memory during model loading

**Attempted Solutions:**

- âœ… Tried q4f32_1 variant (more compatible quantization)
- âœ… Tried q4f16_1 variant (smaller size)
- âœ… Added WebGPU compatibility checks
- âŒ Both variants crash during loading

**Potential Workarounds:**

1. Use a machine with more GPU memory (8GB+ VRAM recommended)
2. Close other browser tabs and applications
3. Wait for WebLLM to release smaller vision models
4. Use a desktop application instead of browser (e.g., MLC-LLM Python)

**Note:** The translation app with Llama 3.2 3B works perfectly because it's significantly smaller and doesn't process images.

## ğŸ¨ UI Design

Both applications feature:

- Modern, clean interfaces inspired by Google Translate
- Responsive design (desktop and mobile)
- Smooth animations and transitions
- Clear visual feedback during loading
- Professional color schemes (blue for translation, purple for vision)

## ğŸ“š Resources

- [WebLLM Documentation](https://webllm.mlc.ai/)
- [WebLLM GitHub](https://github.com/mlc-ai/web-llm)
- [MLC-AI Models](https://huggingface.co/mlc-ai)
- [Phi-3.5-vision Model](https://huggingface.co/mlc-ai/Phi-3.5-vision-instruct-q4f16_1-MLC)

## ğŸš€ Future Improvements

- [ ] Fix vision model memory issues (waiting for smaller models)
- [ ] Add model selection dropdown
- [ ] Implement conversation history for translation
- [ ] Add text-to-speech for translations
- [ ] Support for document translation
- [ ] Batch image processing for vision model
- [ ] Progressive Web App (PWA) support
- [ ] Offline mode

## ğŸ“„ License

This project uses:

- **Next.js:** MIT License
- **WebLLM:** Apache 2.0 License
- **Tailwind CSS:** MIT License

## ğŸ¤ Contributing

This is a demo project showcasing WebLLM capabilities. Feel free to:

- Report issues with browser compatibility
- Suggest UI improvements
- Test on different devices
- Share workarounds for vision model loading

## âš¡ Performance

### Translation App

- **First Load:** ~30-60 seconds (downloads model)
- **Subsequent Loads:** <5 seconds (from cache)
- **Translation Speed:** Real-time streaming (~20-50 tokens/sec)

### Image Describer

- **Status:** Not functional due to model loading issues
- **Expected Performance:** First load ~60-120 seconds, then instant from cache

---

Built with â¤ï¸ using WebLLM, Next.js, and Tailwind CSS
